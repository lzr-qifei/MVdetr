logs/multiviewx/aug_deform_trans_w_dec_lr0.0005_baseR0.1_neck128_out0_alpha1.0_id0_drop0.0_dropcam0.0_worldRK4_10_imgRK12_10_2024-07-04_15-26-39
Settings:
{'reID': False, 'semi_supervised': 0, 'id_ratio': 0, 'cls_thres': 0.6, 'alpha': 1.0, 'use_mse': False, 'arch': 'resnet18', 'dataset': 'multiviewx', 'num_workers': 4, 'batch_size': 1, 'dropout': 0.0, 'dropcam': 0.0, 'epochs': 10, 'lr': 0.0005, 'base_lr_ratio': 0.1, 'weight_decay': 0.0001, 'resume': None, 'visualize': False, 'seed': 2021, 'deterministic': False, 'augmentation': True, 'world_feat': 'deform_trans_w_dec', 'bottleneck_dim': 128, 'outfeat_dim': 0, 'world_reduce': 4, 'world_kernel_size': 10, 'img_reduce': 12, 'img_kernel_size': 10, 'data': '/root/autodl-tmp/MultiviewX'}
rpts:  torch.Size([60000, 6, 4, 2])
pos_emb size:  [ 80 125]
Training...
after merge:  torch.Size([1, 128, 80, 125])
query shape:  torch.Size([1, 200, 128])
ref pts dec:  torch.Size([1, 200, 2])
out_prob shape:  torch.Size([200, 1])
pos_c_cls:  torch.Size([200, 1])
tgt_ids:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])
cost_class shape:  torch.Size([200, 1])
cost_class:  tensor([[1.2635],
        [1.2123],
        [1.2100],
        [1.2536],
        [1.2280],
        [1.2178],
        [1.1728],
        [1.2579],
        [1.2305],
        [1.1897],
        [1.2899],
        [1.3109],
        [1.2321],
        [1.2430],
        [1.2438],
        [1.2073],
        [1.2239],
        [1.2129],
        [1.2333],
        [1.2183],
        [1.2728],
        [1.2388],
        [1.2702],
        [1.2092],
        [1.2460],
        [1.2986],
        [1.2159],
        [1.2521],
        [1.2095],
        [1.2271],
        [1.2192],
        [1.1915],
        [1.2358],
        [1.2699],
        [1.2676],
        [1.2863],
        [1.2668],
        [1.2624],
        [1.2214],
        [1.2437],
        [1.2267],
        [1.2452],
        [1.2355],
        [1.2054],
        [1.2005],
        [1.2792],
        [1.2101],
        [1.2235],
        [1.1610],
        [1.2049],
        [1.2590],
        [1.1959],
        [1.2060],
        [1.2966],
        [1.2730],
        [1.1966],
        [1.1724],
        [1.2285],
        [1.2521],
        [1.2197],
        [1.2214],
        [1.1598],
        [1.2262],
        [1.2003],
        [1.2316],
        [1.2134],
        [1.2698],
        [1.2518],
        [1.2175],
        [1.2350],
        [1.2608],
        [1.2704],
        [1.2171],
        [1.2182],
        [1.2456],
        [1.2390],
        [1.2160],
        [1.2688],
        [1.2831],
        [1.2346],
        [1.1929],
        [1.2537],
        [1.2104],
        [1.2275],
        [1.2119],
        [1.1976],
        [1.3167],
        [1.2485],
        [1.1866],
        [1.2204],
        [1.2240],
        [1.2822],
        [1.2424],
        [1.2268],
        [1.2080],
        [1.2725],
        [1.2769],
        [1.2386],
        [1.1973],
        [1.1918],
        [1.2532],
        [1.2645],
        [1.2102],
        [1.1932],
        [1.2066],
        [1.2208],
        [1.1900],
        [1.1514],
        [1.2543],
        [1.1876],
        [1.2248],
        [1.2377],
        [1.2134],
        [1.2074],
        [1.2791],
        [1.2094],
        [1.2390],
        [1.1962],
        [1.1824],
        [1.2837],
        [1.2560],
        [1.2315],
        [1.2707],
        [1.2338],
        [1.1891],
        [1.2200],
        [1.2364],
        [1.2042],
        [1.2464],
        [1.2230],
        [1.2355],
        [1.2708],
        [1.2448],
        [1.2885],
        [1.2851],
        [1.2295],
        [1.2246],
        [1.1548],
        [1.2623],
        [1.1742],
        [1.2080],
        [1.2634],
        [1.1973],
        [1.2085],
        [1.2195],
        [1.2345],
        [1.2030],
        [1.1964],
        [1.2619],
        [1.2414],
        [1.2165],
        [1.2291],
        [1.2087],
        [1.1820],
        [1.2365],
        [1.1800],
        [1.2140],
        [1.2294],
        [1.2621],
        [1.1841],
        [1.2147],
        [1.1756],
        [1.1757],
        [1.2439],
        [1.2836],
        [1.2264],
        [1.2265],
        [1.2267],
        [1.2518],
        [1.2555],
        [1.2183],
        [1.2140],
        [1.2206],
        [1.2363],
        [1.1829],
        [1.2017],
        [1.1969],
        [1.2443],
        [1.2718],
        [1.2851],
        [1.2186],
        [1.2534],
        [1.2970],
        [1.1964],
        [1.1991],
        [1.2053],
        [1.1554],
        [1.2213],
        [1.2016],
        [1.2545],
        [1.2373],
        [1.2219],
        [1.2401],
        [1.1804],
        [1.2000],
        [1.2413],
        [1.1765],
        [1.1870],
        [1.2312],
        [1.2184]])
cost_pts shape:  torch.Size([200, 41])
cost_pts:  tensor([[0.9120, 0.5258, 0.6286,  ..., 0.4552, 0.6873, 0.9473],
        [1.2463, 0.5921, 0.2264,  ..., 0.3998, 0.4786, 0.7667],
        [0.1768, 0.4774, 1.0402,  ..., 0.7667, 0.5909, 0.5140],
        ...,
        [0.4918, 0.5720, 0.8642,  ..., 0.5907, 0.7335, 0.9935],
        [0.6685, 0.2353, 0.5485,  ..., 0.2751, 0.3968, 0.6568],
        [0.6767, 0.5965, 0.8593,  ..., 0.6672, 0.4350, 0.1751]])
cost torch.Size([200, 41])
indices0 shape:  41
outlabel_shape:  torch.Size([200, 1])
src_logit shape:  torch.Size([200, 1])
tgt_cls shape:  torch.Size([200])
tgt_pts_shape:  torch.Size([41, 2])
{'loss_ce': tensor(4.7907, device='cuda:0', grad_fn=<MulBackward0>), 'loss_center': tensor(0.0007, device='cuda:0', grad_fn=<DivBackward0>)}
after merge:  torch.Size([1, 128, 80, 125])
query shape:  torch.Size([1, 200, 128])
ref pts dec:  torch.Size([1, 200, 2])
out_prob shape:  torch.Size([200, 1])
pos_c_cls:  torch.Size([200, 1])
tgt_ids:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])
cost_class shape:  torch.Size([200, 1])
cost_class:  tensor([[1.1808],
        [1.1452],
        [1.1724],
        [1.2055],
        [1.2023],
        [1.2364],
        [1.1810],
        [1.2180],
        [1.1640],
        [1.2188],
        [1.1891],
        [1.2131],
        [1.1518],
        [1.2319],
        [1.1094],
        [1.1561],
        [1.1444],
        [1.1417],
        [1.2282],
        [1.1999],
        [1.2710],
        [1.1761],
        [1.2013],
        [1.2115],
        [1.1989],
        [1.2236],
        [1.1109],
        [1.2530],
        [1.2050],
        [1.1865],
        [1.1998],
        [1.1890],
        [1.1936],
        [1.1486],
        [1.2062],
        [1.1842],
        [1.1767],
        [1.1952],
        [1.1921],
        [1.1754],
        [1.2552],
        [1.1941],
        [1.1819],
        [1.1393],
        [1.2165],
        [1.2339],
        [1.2709],
        [1.1981],
        [1.1554],
        [1.2085],
        [1.2042],
        [1.1834],
        [1.1650],
        [1.1778],
        [1.2594],
        [1.2073],
        [1.2053],
        [1.1919],
        [1.2202],
        [1.2127],
        [1.1863],
        [1.1736],
        [1.1482],
        [1.2283],
        [1.1979],
        [1.1752],
        [1.1990],
        [1.1852],
        [1.2383],
        [1.2001],
        [1.1722],
        [1.1588],
        [1.2407],
        [1.1875],
        [1.2255],
        [1.1822],
        [1.1976],
        [1.1824],
        [1.2483],
        [1.1821],
        [1.2052],
        [1.1900],
        [1.1880],
        [1.1469],
        [1.2264],
        [1.2380],
        [1.2072],
        [1.1492],
        [1.2053],
        [1.1453],
        [1.1745],
        [1.1984],
        [1.2272],
        [1.2003],
        [1.1617],
        [1.2220],
        [1.1869],
        [1.1845],
        [1.1716],
        [1.1758],
        [1.1510],
        [1.1559],
        [1.1770],
        [1.1463],
        [1.2152],
        [1.1329],
        [1.1458],
        [1.1849],
        [1.1915],
        [1.1775],
        [1.1333],
        [1.1917],
        [1.1869],
        [1.1822],
        [1.1379],
        [1.1734],
        [1.1317],
        [1.2213],
        [1.2176],
        [1.2095],
        [1.1926],
        [1.1664],
        [1.1429],
        [1.2028],
        [1.1607],
        [1.1707],
        [1.2198],
        [1.2311],
        [1.1341],
        [1.1693],
        [1.1326],
        [1.1834],
        [1.1909],
        [1.1529],
        [1.1899],
        [1.2216],
        [1.1882],
        [1.1484],
        [1.1702],
        [1.1257],
        [1.1664],
        [1.1352],
        [1.2185],
        [1.1851],
        [1.2027],
        [1.1435],
        [1.2255],
        [1.1626],
        [1.2458],
        [1.2017],
        [1.1806],
        [1.2108],
        [1.2214],
        [1.2021],
        [1.1269],
        [1.2384],
        [1.2054],
        [1.2306],
        [1.1344],
        [1.2139],
        [1.2182],
        [1.2409],
        [1.1935],
        [1.1700],
        [1.1782],
        [1.2028],
        [1.2570],
        [1.1940],
        [1.2004],
        [1.1966],
        [1.1553],
        [1.2319],
        [1.2071],
        [1.1898],
        [1.2005],
        [1.2543],
        [1.1786],
        [1.1296],
        [1.1291],
        [1.2026],
        [1.1720],
        [1.1904],
        [1.1927],
        [1.2220],
        [1.1599],
        [1.2230],
        [1.2415],
        [1.2601],
        [1.1634],
        [1.1932],
        [1.2264],
        [1.2151],
        [1.1921],
        [1.1742],
        [1.1693],
        [1.2258],
        [1.1977],
        [1.1553],
        [1.1622],
        [1.2316]])
cost_pts shape:  torch.Size([200, 39])
cost_pts:  tensor([[0.5381, 0.5778, 0.7302,  ..., 0.1302, 0.7096, 0.8363],
        [0.7419, 0.2772, 1.0646,  ..., 0.8256, 0.4603, 0.8776],
        [0.3276, 0.9754, 0.0181,  ..., 0.6050, 0.6092, 0.4031],
        ...,
        [0.5843, 0.7994, 0.4976,  ..., 0.4290, 0.7558, 0.8825],
        [0.2476, 0.4837, 0.4867,  ..., 0.2610, 0.4192, 0.5458],
        [0.5843, 0.7945, 0.6710,  ..., 1.0929, 0.4283, 0.2860]])
cost torch.Size([200, 39])
indices0 shape:  39
outlabel_shape:  torch.Size([200, 1])
src_logit shape:  torch.Size([200, 1])
tgt_cls shape:  torch.Size([200])
tgt_pts_shape:  torch.Size([39, 2])
{'loss_ce': tensor(4.9409, device='cuda:0', grad_fn=<MulBackward0>), 'loss_center': tensor(0.0007, device='cuda:0', grad_fn=<DivBackward0>)}
after merge:  torch.Size([1, 128, 80, 125])
query shape:  torch.Size([1, 200, 128])
ref pts dec:  torch.Size([1, 200, 2])
out_prob shape:  torch.Size([200, 1])
pos_c_cls:  torch.Size([200, 1])
tgt_ids:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])
cost_class shape:  torch.Size([200, 1])
cost_class:  tensor([[1.2629],
        [1.2067],
        [1.1967],
        [1.1849],
        [1.2192],
        [1.1633],
        [1.2360],
        [1.1504],
        [1.1664],
        [1.2245],
        [1.2629],
        [1.2318],
        [1.1991],
        [1.2354],
        [1.2393],
        [1.2286],
        [1.2154],
        [1.2297],
        [1.2176],
        [1.1980],
        [1.1843],
        [1.1784],
        [1.2112],
        [1.2140],
        [1.2112],
        [1.2319],
        [1.2174],
        [1.1858],
        [1.2223],
        [1.2388],
        [1.2104],
        [1.2106],
        [1.2076],
        [1.2167],
        [1.2412],
        [1.1916],
        [1.1968],
        [1.2389],
        [1.2382],
        [1.2208],
        [1.2255],
        [1.2393],
        [1.2274],
        [1.2268],
        [1.2268],
        [1.2247],
        [1.2497],
        [1.1726],
        [1.2059],
        [1.1813],
        [1.2018],
        [1.2272],
        [1.2075],
        [1.2836],
        [1.2106],
        [1.1500],
        [1.2303],
        [1.2493],
        [1.2557],
        [1.2478],
        [1.2089],
        [1.1917],
        [1.2216],
        [1.1673],
        [1.2416],
        [1.2523],
        [1.1719],
        [1.1719],
        [1.2065],
        [1.2832],
        [1.2598],
        [1.2709],
        [1.1389],
        [1.2329],
        [1.1865],
        [1.2807],
        [1.2353],
        [1.1754],
        [1.2227],
        [1.2321],
        [1.1931],
        [1.2399],
        [1.1767],
        [1.2371],
        [1.1724],
        [1.1874],
        [1.2212],
        [1.2562],
        [1.2732],
        [1.1745],
        [1.2469],
        [1.1842],
        [1.2019],
        [1.2274],
        [1.2152],
        [1.2503],
        [1.2558],
        [1.1648],
        [1.2212],
        [1.2211],
        [1.2016],
        [1.2323],
        [1.2509],
        [1.2529],
        [1.2066],
        [1.1657],
        [1.1783],
        [1.2253],
        [1.1285],
        [1.1726],
        [1.2267],
        [1.2202],
        [1.1879],
        [1.2940],
        [1.2750],
        [1.2404],
        [1.1496],
        [1.2433],
        [1.2295],
        [1.2268],
        [1.1945],
        [1.1892],
        [1.1869],
        [1.2507],
        [1.2526],
        [1.2060],
        [1.1997],
        [1.2008],
        [1.2403],
        [1.2171],
        [1.1727],
        [1.2231],
        [1.2259],
        [1.2009],
        [1.1914],
        [1.2570],
        [1.2829],
        [1.2877],
        [1.2199],
        [1.2527],
        [1.2081],
        [1.1873],
        [1.1959],
        [1.2482],
        [1.2103],
        [1.2420],
        [1.2231],
        [1.2538],
        [1.2095],
        [1.2270],
        [1.2350],
        [1.1972],
        [1.2281],
        [1.2385],
        [1.1833],
        [1.2187],
        [1.2188],
        [1.2168],
        [1.2262],
        [1.1856],
        [1.1873],
        [1.2334],
        [1.2438],
        [1.2406],
        [1.1633],
        [1.2604],
        [1.2000],
        [1.2220],
        [1.2042],
        [1.2410],
        [1.1926],
        [1.2348],
        [1.2110],
        [1.2419],
        [1.2538],
        [1.2163],
        [1.2332],
        [1.2235],
        [1.2493],
        [1.2407],
        [1.2666],
        [1.2752],
        [1.1925],
        [1.2299],
        [1.2744],
        [1.1773],
        [1.1759],
        [1.2615],
        [1.2427],
        [1.1950],
        [1.2250],
        [1.2684],
        [1.2349],
        [1.2354],
        [1.2285],
        [1.2081],
        [1.2403],
        [1.2272],
        [1.1536],
        [1.2755]])
cost_pts shape:  torch.Size([200, 37])
cost_pts:  tensor([[0.4463, 0.5807, 0.5557,  ..., 0.5664, 0.9143, 0.7818],
        [0.7157, 0.2743, 0.8900,  ..., 0.2887, 0.5056, 1.1161],
        [0.3538, 0.9162, 0.1795,  ..., 0.8399, 0.5639, 0.2666],
        ...,
        [0.4925, 0.7402, 0.5262,  ..., 0.6639, 0.9605, 0.7460],
        [0.1558, 0.4246, 0.3122,  ..., 0.3483, 0.6238, 0.5383],
        [0.6761, 0.7353, 0.6424,  ..., 0.6590, 0.3830, 0.4225]])
cost torch.Size([200, 37])
indices0 shape:  37
outlabel_shape:  torch.Size([200, 1])
src_logit shape:  torch.Size([200, 1])
tgt_cls shape:  torch.Size([200])
tgt_pts_shape:  torch.Size([37, 2])
{'loss_ce': tensor(5.3959, device='cuda:0', grad_fn=<MulBackward0>), 'loss_center': tensor(0.0008, device='cuda:0', grad_fn=<DivBackward0>)}
after merge:  torch.Size([1, 128, 80, 125])
query shape:  torch.Size([1, 200, 128])
ref pts dec:  torch.Size([1, 200, 2])
out_prob shape:  torch.Size([200, 1])
pos_c_cls:  torch.Size([200, 1])
tgt_ids:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])
cost_class shape:  torch.Size([200, 1])
cost_class:  tensor([[1.2668],
        [1.2736],
        [1.2273],
        [1.2609],
        [1.2943],
        [1.2576],
        [1.2778],
        [1.2313],
        [1.2136],
        [1.2604],
        [1.2763],
        [1.2644],
        [1.2670],
        [1.2419],
        [1.2273],
        [1.3196],
        [1.2531],
        [1.3125],
        [1.2362],
        [1.2368],
        [1.2879],
        [1.2417],
        [1.2927],
        [1.2474],
        [1.2682],
        [1.2046],
        [1.2978],
        [1.3003],
        [1.2172],
        [1.2834],
        [1.1988],
        [1.2454],
        [1.2603],
        [1.2683],
        [1.3242],
        [1.2581],
        [1.2515],
        [1.2741],
        [1.2465],
        [1.2799],
        [1.2472],
        [1.3040],
        [1.2977],
        [1.2819],
        [1.2936],
        [1.2387],
        [1.2786],
        [1.2423],
        [1.2471],
        [1.2566],
        [1.2405],
        [1.2762],
        [1.2116],
        [1.2752],
        [1.2935],
        [1.2949],
        [1.3266],
        [1.2563],
        [1.2847],
        [1.2953],
        [1.3127],
        [1.2671],
        [1.2069],
        [1.2430],
        [1.2505],
        [1.3224],
        [1.2487],
        [1.2509],
        [1.2893],
        [1.2432],
        [1.2497],
        [1.2502],
        [1.2851],
        [1.3165],
        [1.2304],
        [1.2837],
        [1.2464],
        [1.2754],
        [1.2319],
        [1.2596],
        [1.2340],
        [1.2670],
        [1.3278],
        [1.2676],
        [1.2979],
        [1.2644],
        [1.2416],
        [1.2862],
        [1.2662],
        [1.2773],
        [1.2688],
        [1.2900],
        [1.2697],
        [1.2800],
        [1.2352],
        [1.3016],
        [1.2190],
        [1.2844],
        [1.2306],
        [1.2492],
        [1.2960],
        [1.2458],
        [1.2911],
        [1.2360],
        [1.3047],
        [1.2557],
        [1.2350],
        [1.2496],
        [1.2873],
        [1.2949],
        [1.2181],
        [1.2567],
        [1.2669],
        [1.2408],
        [1.2857],
        [1.1764],
        [1.2676],
        [1.3332],
        [1.2664],
        [1.3267],
        [1.2767],
        [1.3034],
        [1.2338],
        [1.2319],
        [1.2283],
        [1.2537],
        [1.2954],
        [1.3510],
        [1.2350],
        [1.2546],
        [1.2528],
        [1.2399],
        [1.2831],
        [1.2644],
        [1.2879],
        [1.2674],
        [1.2614],
        [1.2518],
        [1.2559],
        [1.2428],
        [1.2368],
        [1.2012],
        [1.2101],
        [1.2755],
        [1.2400],
        [1.2330],
        [1.2718],
        [1.2434],
        [1.2933],
        [1.2755],
        [1.2248],
        [1.2729],
        [1.2036],
        [1.2590],
        [1.2596],
        [1.2551],
        [1.2635],
        [1.2252],
        [1.2929],
        [1.2063],
        [1.2881],
        [1.2877],
        [1.2471],
        [1.2252],
        [1.2251],
        [1.2226],
        [1.2658],
        [1.2163],
        [1.2737],
        [1.2652],
        [1.2914],
        [1.2044],
        [1.3057],
        [1.3034],
        [1.3467],
        [1.2743],
        [1.3028],
        [1.2351],
        [1.2485],
        [1.2483],
        [1.1826],
        [1.3030],
        [1.2500],
        [1.2645],
        [1.2608],
        [1.2685],
        [1.2373],
        [1.2425],
        [1.2399],
        [1.2564],
        [1.3303],
        [1.2789],
        [1.2920],
        [1.2142],
        [1.3011],
        [1.2837],
        [1.2665],
        [1.2730],
        [1.2366],
        [1.3170]])
cost_pts shape:  torch.Size([200, 38])
cost_pts:  tensor([[0.5669, 0.4891, 0.4333,  ..., 0.6432, 0.8694, 0.7300],
        [0.8551, 0.3659, 0.6527,  ..., 0.4408, 0.6326, 1.0619],
        [0.2144, 0.7547, 0.4168,  ..., 0.6287, 0.4369, 0.2968],
        ...,
        [0.6131, 0.5787, 0.4795,  ..., 0.6894, 0.9156, 0.7762],
        [0.2772, 0.2630, 0.1428,  ..., 0.3527, 0.5789, 0.4841],
        [0.5555, 0.6332, 0.6891,  ..., 0.4792, 0.2560, 0.3924]])
cost torch.Size([200, 38])
indices0 shape:  38
outlabel_shape:  torch.Size([200, 1])
src_logit shape:  torch.Size([200, 1])
tgt_cls shape:  torch.Size([200])
tgt_pts_shape:  torch.Size([38, 2])
{'loss_ce': tensor(5.4071, device='cuda:0', grad_fn=<MulBackward0>), 'loss_center': tensor(0.0007, device='cuda:0', grad_fn=<DivBackward0>)}
after merge:  torch.Size([1, 128, 80, 125])
query shape:  torch.Size([1, 200, 128])
ref pts dec:  torch.Size([1, 200, 2])
out_prob shape:  torch.Size([200, 1])
pos_c_cls:  torch.Size([200, 1])
tgt_ids:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])
cost_class shape:  torch.Size([200, 1])
cost_class:  tensor([[1.1893],
        [1.1692],
        [1.1822],
        [1.1671],
        [1.1953],
        [1.1561],
        [1.2133],
        [1.2139],
        [1.1793],
        [1.1778],
        [1.1567],
        [1.1261],
        [1.1559],
        [1.1335],
        [1.2282],
        [1.1446],
        [1.1749],
        [1.1355],
        [1.1858],
        [1.2121],
        [1.1354],
        [1.1716],
        [1.1912],
        [1.2148],
        [1.2056],
        [1.1652],
        [1.2035],
        [1.1500],
        [1.2104],
        [1.2046],
        [1.1648],
        [1.1747],
        [1.1937],
        [1.1696],
        [1.1876],
        [1.1674],
        [1.1248],
        [1.2053],
        [1.1944],
        [1.1669],
        [1.1910],
        [1.1798],
        [1.1656],
        [1.1972],
        [1.1559],
        [1.2094],
        [1.2016],
        [1.1817],
        [1.1527],
        [1.1530],
        [1.1915],
        [1.1825],
        [1.2118],
        [1.1386],
        [1.2113],
        [1.1991],
        [1.1717],
        [1.1832],
        [1.1961],
        [1.1557],
        [1.1319],
        [1.1632],
        [1.1895],
        [1.1829],
        [1.2454],
        [1.2099],
        [1.2465],
        [1.1821],
        [1.2058],
        [1.1898],
        [1.1777],
        [1.1711],
        [1.1728],
        [1.2108],
        [1.1935],
        [1.1989],
        [1.1558],
        [1.2124],
        [1.1962],
        [1.1185],
        [1.1795],
        [1.1893],
        [1.1325],
        [1.1683],
        [1.2500],
        [1.1769],
        [1.2010],
        [1.1703],
        [1.1904],
        [1.1678],
        [1.1409],
        [1.1831],
        [1.1915],
        [1.1562],
        [1.2612],
        [1.2412],
        [1.1809],
        [1.2207],
        [1.1499],
        [1.1605],
        [1.1430],
        [1.1663],
        [1.1695],
        [1.1899],
        [1.1936],
        [1.1294],
        [1.2051],
        [1.2187],
        [1.1858],
        [1.1537],
        [1.1417],
        [1.2115],
        [1.1868],
        [1.1501],
        [1.1620],
        [1.1669],
        [1.1709],
        [1.1982],
        [1.2160],
        [1.1777],
        [1.1958],
        [1.1732],
        [1.2163],
        [1.1345],
        [1.1747],
        [1.2094],
        [1.1379],
        [1.1974],
        [1.2044],
        [1.2519],
        [1.1569],
        [1.1962],
        [1.2055],
        [1.1780],
        [1.1894],
        [1.1780],
        [1.1550],
        [1.1556],
        [1.1643],
        [1.2087],
        [1.2327],
        [1.2324],
        [1.1376],
        [1.1464],
        [1.1957],
        [1.1626],
        [1.1606],
        [1.1565],
        [1.1965],
        [1.1585],
        [1.1816],
        [1.1379],
        [1.2208],
        [1.1569],
        [1.1954],
        [1.1897],
        [1.1933],
        [1.1534],
        [1.2393],
        [1.2319],
        [1.1605],
        [1.2362],
        [1.1757],
        [1.1488],
        [1.1805],
        [1.1958],
        [1.1887],
        [1.1759],
        [1.2281],
        [1.2139],
        [1.1144],
        [1.1808],
        [1.2127],
        [1.1501],
        [1.1840],
        [1.1896],
        [1.1318],
        [1.1854],
        [1.1671],
        [1.1619],
        [1.1547],
        [1.1954],
        [1.2059],
        [1.2118],
        [1.1645],
        [1.1439],
        [1.2466],
        [1.2401],
        [1.2266],
        [1.1892],
        [1.1973],
        [1.1856],
        [1.1363],
        [1.1849],
        [1.1927],
        [1.2253],
        [1.1550],
        [1.1960],
        [1.1736],
        [1.1575]])
cost_pts shape:  torch.Size([200, 39])
cost_pts:  tensor([[0.6431, 0.4465, 0.7798,  ..., 0.1833, 0.6463, 0.9674],
        [0.9774, 0.5815, 1.1142,  ..., 0.9287, 0.2657, 0.7306],
        [0.1373, 0.4880, 0.0447,  ..., 0.5519, 0.8038, 0.5341],
        ...,
        [0.6167, 0.4927, 0.5160,  ..., 0.3759, 0.6925, 1.0136],
        [0.3996, 0.1560, 0.5364,  ..., 0.3642, 0.3558, 0.6769],
        [0.5519, 0.6759, 0.6526,  ..., 1.1960, 0.6229, 0.1580]])
cost torch.Size([200, 39])
indices0 shape:  39
outlabel_shape:  torch.Size([200, 1])
src_logit shape:  torch.Size([200, 1])
tgt_cls shape:  torch.Size([200])
tgt_pts_shape:  torch.Size([39, 2])
{'loss_ce': tensor(4.9018, device='cuda:0', grad_fn=<MulBackward0>), 'loss_center': tensor(0.0007, device='cuda:0', grad_fn=<DivBackward0>)}
after merge:  torch.Size([1, 128, 80, 125])
query shape:  torch.Size([1, 200, 128])
ref pts dec:  torch.Size([1, 200, 2])
out_prob shape:  torch.Size([200, 1])
pos_c_cls:  torch.Size([200, 1])
tgt_ids:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])
cost_class shape:  torch.Size([200, 1])
cost_class:  tensor([[1.1694],
        [1.0738],
        [1.1193],
        [1.0707],
        [1.0649],
        [1.1075],
        [1.0927],
        [1.0994],
        [1.0826],
        [1.0824],
        [1.0845],
        [1.0789],
        [1.1560],
        [1.1260],
        [1.1685],
        [1.0965],
        [1.1389],
        [1.1502],
        [1.0065],
        [1.0839],
        [1.1199],
        [1.1525],
        [1.1708],
        [1.0635],
        [1.0463],
        [1.1051],
        [1.0616],
        [1.0674],
        [1.0938],
        [1.0780],
        [1.1397],
        [1.0980],
        [1.0079],
        [1.1147],
        [1.0951],
        [1.1162],
        [1.0976],
        [1.1216],
        [1.0671],
        [1.1357],
        [1.1091],
        [1.0702],
        [1.1329],
        [1.1013],
        [1.0970],
        [1.1194],
        [1.1047],
        [1.0780],
        [1.0969],
        [1.1146],
        [1.0794],
        [1.1442],
        [1.1242],
        [1.0790],
        [1.1111],
        [1.1062],
        [1.0512],
        [1.1451],
        [1.0549],
        [1.1635],
        [1.1360],
        [1.1233],
        [1.1619],
        [1.1267],
        [1.1164],
        [1.0935],
        [1.0722],
        [1.1057],
        [1.0830],
        [1.1095],
        [1.1596],
        [1.1101],
        [1.1434],
        [1.1029],
        [1.1034],
        [1.0823],
        [1.0879],
        [1.1017],
        [1.1492],
        [1.0890],
        [1.1360],
        [1.1308],
        [1.1763],
        [1.1331],
        [1.0724],
        [1.1059],
        [1.0946],
        [1.1358],
        [1.1015],
        [1.1291],
        [1.1031],
        [1.0964],
        [1.0988],
        [1.0923],
        [1.0869],
        [1.0916],
        [1.0895],
        [1.1933],
        [1.0728],
        [1.1242],
        [1.0934],
        [1.0752],
        [1.1230],
        [1.0973],
        [1.0695],
        [1.1123],
        [1.1054],
        [1.0909],
        [1.1022],
        [1.1106],
        [1.0872],
        [1.0437],
        [1.1077],
        [1.0537],
        [1.1157],
        [1.0476],
        [1.1097],
        [1.1102],
        [1.1154],
        [1.1733],
        [1.0827],
        [1.1426],
        [1.0927],
        [1.1183],
        [1.0945],
        [1.1120],
        [1.0695],
        [1.0560],
        [1.1377],
        [1.0423],
        [1.1418],
        [1.1282],
        [1.0674],
        [1.0754],
        [1.0463],
        [1.0743],
        [1.1173],
        [1.0455],
        [1.1195],
        [1.0852],
        [1.0702],
        [1.1370],
        [1.1095],
        [1.1037],
        [1.1253],
        [1.1093],
        [1.0679],
        [1.0644],
        [1.1044],
        [1.1096],
        [1.1366],
        [1.0972],
        [1.1279],
        [1.1272],
        [1.0749],
        [1.1213],
        [1.0696],
        [1.1035],
        [1.0547],
        [1.1237],
        [1.1338],
        [1.1424],
        [1.0660],
        [1.0805],
        [1.0663],
        [1.0851],
        [1.0932],
        [1.0616],
        [1.1353],
        [1.0823],
        [1.1025],
        [1.1352],
        [1.0562],
        [1.0775],
        [1.0924],
        [1.0602],
        [1.0961],
        [1.1195],
        [1.1251],
        [1.1121],
        [1.0973],
        [1.0715],
        [1.0967],
        [1.0739],
        [1.0575],
        [1.0781],
        [1.1133],
        [1.1236],
        [1.1369],
        [1.1037],
        [1.1509],
        [1.1170],
        [1.1468],
        [1.0564],
        [1.0465],
        [1.1903],
        [1.0637],
        [1.0819],
        [1.0843],
        [1.1174]])
cost_pts shape:  torch.Size([200, 39])
cost_pts:  tensor([[0.9984, 0.6208, 0.7305,  ..., 0.1450, 0.6443, 0.7436],
        [1.3328, 0.6652, 0.1246,  ..., 0.8264, 0.3137, 1.0724],
        [0.2633, 0.4043, 1.0540,  ..., 0.5902, 0.7558, 0.3103],
        ...,
        [0.4393, 0.6670, 0.8780,  ..., 0.4142, 0.6905, 0.7898],
        [0.7549, 0.3303, 0.5623,  ..., 0.2618, 0.3538, 0.4946],
        [0.7872, 0.5016, 0.8731,  ..., 1.0937, 0.5749, 0.3788]])
cost torch.Size([200, 39])
indices0 shape:  39
outlabel_shape:  torch.Size([200, 1])
src_logit shape:  torch.Size([200, 1])
tgt_cls shape:  torch.Size([200])
tgt_pts_shape:  torch.Size([39, 2])
{'loss_ce': tensor(4.5761, device='cuda:0', grad_fn=<MulBackward0>), 'loss_center': tensor(0.0008, device='cuda:0', grad_fn=<DivBackward0>)}
after merge:  torch.Size([1, 128, 80, 125])
query shape:  torch.Size([1, 200, 128])
ref pts dec:  torch.Size([1, 200, 2])
out_prob shape:  torch.Size([200, 1])
pos_c_cls:  torch.Size([200, 1])
tgt_ids:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])
cost_class shape:  torch.Size([200, 1])
cost_class:  tensor([[1.1776],
        [1.1159],
        [1.1480],
        [1.0980],
        [1.0490],
        [1.1263],
        [1.0862],
        [1.1905],
        [1.1648],
        [1.1587],
        [1.1680],
        [1.1630],
        [1.0987],
        [1.0903],
        [1.1543],
        [1.1243],
        [1.0868],
        [1.0546],
        [1.1060],
        [1.1257],
        [1.1023],
        [1.1138],
        [1.1242],
        [1.1154],
        [1.1719],
        [1.1562],
        [1.1288],
        [1.1003],
        [1.1671],
        [1.1055],
        [1.1269],
        [1.1014],
        [1.1483],
        [1.1586],
        [1.1602],
        [1.1243],
        [1.1767],
        [1.1422],
        [1.1310],
        [1.1062],
        [1.1345],
        [1.0929],
        [1.1388],
        [1.1456],
        [1.1355],
        [1.0754],
        [1.1096],
        [1.0938],
        [1.1296],
        [1.1370],
        [1.1456],
        [1.1588],
        [1.1129],
        [1.1898],
        [1.0795],
        [1.1627],
        [1.1472],
        [1.1420],
        [1.1309],
        [1.1255],
        [1.1620],
        [1.1352],
        [1.1414],
        [1.1282],
        [1.1133],
        [1.1205],
        [1.1110],
        [1.1516],
        [1.1271],
        [1.1337],
        [1.0941],
        [1.1024],
        [1.1598],
        [1.1883],
        [1.1121],
        [1.1298],
        [1.1720],
        [1.1523],
        [1.1224],
        [1.1064],
        [1.1505],
        [1.1371],
        [1.0407],
        [1.1121],
        [1.1448],
        [1.1423],
        [1.1413],
        [1.1483],
        [1.1088],
        [1.1356],
        [1.1284],
        [1.0893],
        [1.1564],
        [1.1391],
        [1.0991],
        [1.0696],
        [1.1733],
        [1.0799],
        [1.1727],
        [1.1049],
        [1.1722],
        [1.1417],
        [1.1328],
        [1.1481],
        [1.1829],
        [1.1604],
        [1.1697],
        [1.0851],
        [1.0961],
        [1.1551],
        [1.1462],
        [1.1209],
        [1.1191],
        [1.1807],
        [1.1402],
        [1.0659],
        [1.1305],
        [1.1367],
        [1.0849],
        [1.1383],
        [1.1201],
        [1.1587],
        [1.1322],
        [1.1039],
        [1.1504],
        [1.1186],
        [1.1411],
        [1.1126],
        [1.1546],
        [1.1625],
        [1.1400],
        [1.1473],
        [1.1351],
        [1.1307],
        [1.1475],
        [1.1583],
        [1.1861],
        [1.2299],
        [1.1319],
        [1.1142],
        [1.1309],
        [1.0951],
        [1.1179],
        [1.1293],
        [1.1450],
        [1.1602],
        [1.1472],
        [1.1308],
        [1.1722],
        [1.1527],
        [1.1227],
        [1.1540],
        [1.0741],
        [1.1752],
        [1.1407],
        [1.0659],
        [1.1635],
        [1.1455],
        [1.1211],
        [1.1898],
        [1.0739],
        [1.1554],
        [1.0913],
        [1.1190],
        [1.1251],
        [1.0984],
        [1.1572],
        [1.1510],
        [1.1871],
        [1.1648],
        [1.1253],
        [1.1816],
        [1.1264],
        [1.1281],
        [1.1059],
        [1.1419],
        [1.0947],
        [1.2016],
        [1.1370],
        [1.1098],
        [1.0820],
        [1.1801],
        [1.1176],
        [1.1144],
        [1.2015],
        [1.0803],
        [1.1361],
        [1.0672],
        [1.1719],
        [1.1927],
        [1.1593],
        [1.1573],
        [1.1415],
        [1.1084],
        [1.1497],
        [1.1697],
        [1.1234],
        [1.1021],
        [1.1290],
        [1.1269]])
cost_pts shape:  torch.Size([200, 41])
cost_pts:  tensor([[0.6613, 0.4526, 0.5814,  ..., 0.6908, 0.8648, 1.0006],
        [0.9957, 0.4025, 0.9158,  ..., 0.4071, 0.8092, 1.3349],
        [0.1430, 0.6881, 0.1537,  ..., 0.6624, 0.4315, 0.2655],
        ...,
        [0.6225, 0.5121, 0.3644,  ..., 0.7370, 0.9110, 0.5492],
        [0.4179, 0.1964, 0.3379,  ..., 0.4003, 0.5743, 0.7571],
        [0.5461, 0.6698, 0.8042,  ..., 0.4815, 0.2576, 0.6194]])
cost torch.Size([200, 41])
indices0 shape:  41
outlabel_shape:  torch.Size([200, 1])
src_logit shape:  torch.Size([200, 1])
tgt_cls shape:  torch.Size([200])
tgt_pts_shape:  torch.Size([41, 2])
{'loss_ce': tensor(4.4106, device='cuda:0', grad_fn=<MulBackward0>), 'loss_center': tensor(0.0007, device='cuda:0', grad_fn=<DivBackward0>)}
after merge:  torch.Size([1, 128, 80, 125])
query shape:  torch.Size([1, 200, 128])
ref pts dec:  torch.Size([1, 200, 2])
out_prob shape:  torch.Size([200, 1])
pos_c_cls:  torch.Size([200, 1])
tgt_ids:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])
cost_class shape:  torch.Size([200, 1])
cost_class:  tensor([[1.3227],
        [1.3123],
        [1.3384],
        [1.3236],
        [1.3058],
        [1.3346],
        [1.2707],
        [1.2850],
        [1.3267],
        [1.3045],
        [1.3146],
        [1.2817],
        [1.3390],
        [1.3043],
        [1.2682],
        [1.3188],
        [1.3689],
        [1.3460],
        [1.3092],
        [1.3508],
        [1.3392],
        [1.3518],
        [1.3664],
        [1.2958],
        [1.3058],
        [1.3361],
        [1.3210],
        [1.3067],
        [1.3150],
        [1.3344],
        [1.3638],
        [1.2991],
        [1.3495],
        [1.2372],
        [1.3120],
        [1.3226],
        [1.3049],
        [1.3242],
        [1.3395],
        [1.2935],
        [1.3062],
        [1.3086],
        [1.2935],
        [1.2828],
        [1.3436],
        [1.3536],
        [1.3038],
        [1.2993],
        [1.3224],
        [1.2809],
        [1.2629],
        [1.3077],
        [1.2718],
        [1.3132],
        [1.2834],
        [1.3251],
        [1.3205],
        [1.3304],
        [1.2741],
        [1.3012],
        [1.3186],
        [1.3066],
        [1.3079],
        [1.3499],
        [1.3345],
        [1.3196],
        [1.2617],
        [1.3401],
        [1.3061],
        [1.3016],
        [1.3560],
        [1.3138],
        [1.3250],
        [1.3150],
        [1.3194],
        [1.3088],
        [1.3473],
        [1.3165],
        [1.3387],
        [1.3877],
        [1.3406],
        [1.2876],
        [1.3515],
        [1.2965],
        [1.3239],
        [1.3338],
        [1.2866],
        [1.2912],
        [1.2641],
        [1.3348],
        [1.3424],
        [1.3646],
        [1.3040],
        [1.3389],
        [1.3158],
        [1.3297],
        [1.2630],
        [1.3304],
        [1.3332],
        [1.2448],
        [1.2928],
        [1.2914],
        [1.2861],
        [1.2592],
        [1.3063],
        [1.3270],
        [1.2873],
        [1.3091],
        [1.3264],
        [1.2835],
        [1.3105],
        [1.3003],
        [1.2959],
        [1.3069],
        [1.3033],
        [1.3271],
        [1.3358],
        [1.3099],
        [1.3148],
        [1.2936],
        [1.3274],
        [1.2967],
        [1.3185],
        [1.3000],
        [1.3403],
        [1.3050],
        [1.3078],
        [1.3293],
        [1.2819],
        [1.3667],
        [1.2907],
        [1.3297],
        [1.3212],
        [1.3692],
        [1.3837],
        [1.2811],
        [1.2739],
        [1.3135],
        [1.3723],
        [1.2877],
        [1.2989],
        [1.2951],
        [1.3178],
        [1.3125],
        [1.3224],
        [1.3578],
        [1.3681],
        [1.3846],
        [1.3474],
        [1.2973],
        [1.2841],
        [1.3280],
        [1.2901],
        [1.3024],
        [1.3492],
        [1.3041],
        [1.3100],
        [1.3454],
        [1.2781],
        [1.3443],
        [1.3167],
        [1.2962],
        [1.2797],
        [1.3406],
        [1.3454],
        [1.2613],
        [1.2711],
        [1.3060],
        [1.3492],
        [1.3002],
        [1.3212],
        [1.2817],
        [1.3372],
        [1.3194],
        [1.3047],
        [1.3199],
        [1.3513],
        [1.3076],
        [1.2894],
        [1.3191],
        [1.3430],
        [1.3748],
        [1.3337],
        [1.3514],
        [1.3250],
        [1.3489],
        [1.3060],
        [1.3177],
        [1.2794],
        [1.3246],
        [1.3320],
        [1.2958],
        [1.2755],
        [1.3086],
        [1.3379],
        [1.3180],
        [1.3250],
        [1.3152],
        [1.3322],
        [1.2569]])
cost_pts shape:  torch.Size([200, 35])
cost_pts:  tensor([[0.8629, 0.5835, 0.6796,  ..., 0.9328, 0.4115, 0.9303],
        [1.1973, 0.7904, 0.1755,  ..., 1.2671, 0.4436, 0.7216],
        [0.1278, 0.2790, 1.0611,  ..., 0.1976, 0.8230, 0.4971],
        ...,
        [0.4709, 0.6297, 0.8851,  ..., 0.5970, 0.6470, 0.9765],
        [0.6194, 0.2930, 0.5694,  ..., 0.6893, 0.3313, 0.6398],
        [0.6977, 0.5389, 0.8802,  ..., 0.5715, 0.7109, 0.1920]])
cost torch.Size([200, 35])
indices0 shape:  35
outlabel_shape:  torch.Size([200, 1])
src_logit shape:  torch.Size([200, 1])
tgt_cls shape:  torch.Size([200])
tgt_pts_shape:  torch.Size([35, 2])
{'loss_ce': tensor(6.2202, device='cuda:0', grad_fn=<MulBackward0>), 'loss_center': tensor(0.0007, device='cuda:0', grad_fn=<DivBackward0>)}
after merge:  torch.Size([1, 128, 80, 125])
query shape:  torch.Size([1, 200, 128])
ref pts dec:  torch.Size([1, 200, 2])
out_prob shape:  torch.Size([200, 1])
pos_c_cls:  torch.Size([200, 1])
tgt_ids:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])
cost_class shape:  torch.Size([200, 1])
cost_class:  tensor([[1.1604],
        [1.1814],
        [1.1697],
        [1.2135],
        [1.1300],
        [1.2321],
        [1.1737],
        [1.2002],
        [1.2036],
        [1.1724],
        [1.2138],
        [1.1444],
        [1.1794],
        [1.1839],
        [1.2029],
        [1.1547],
        [1.1511],
        [1.1694],
        [1.1669],
        [1.2186],
        [1.1614],
        [1.2005],
        [1.1979],
        [1.1564],
        [1.2035],
        [1.2307],
        [1.1805],
        [1.1741],
        [1.2150],
        [1.1403],
        [1.2173],
        [1.1818],
        [1.2519],
        [1.1770],
        [1.1631],
        [1.1431],
        [1.2379],
        [1.2165],
        [1.2076],
        [1.1578],
        [1.1709],
        [1.1817],
        [1.1746],
        [1.1651],
        [1.1895],
        [1.1906],
        [1.1431],
        [1.1887],
        [1.1673],
        [1.1514],
        [1.2029],
        [1.2323],
        [1.2186],
        [1.1451],
        [1.1970],
        [1.1916],
        [1.2007],
        [1.1986],
        [1.1678],
        [1.1622],
        [1.1475],
        [1.1733],
        [1.1900],
        [1.2329],
        [1.1646],
        [1.1851],
        [1.2069],
        [1.1866],
        [1.1799],
        [1.1853],
        [1.1759],
        [1.1923],
        [1.1809],
        [1.2377],
        [1.1902],
        [1.1683],
        [1.2483],
        [1.1693],
        [1.2312],
        [1.2468],
        [1.1963],
        [1.1986],
        [1.1993],
        [1.2099],
        [1.1650],
        [1.2145],
        [1.1948],
        [1.2399],
        [1.1440],
        [1.2003],
        [1.2236],
        [1.2093],
        [1.1422],
        [1.2073],
        [1.1654],
        [1.1897],
        [1.1763],
        [1.1779],
        [1.1786],
        [1.2023],
        [1.1858],
        [1.1592],
        [1.1573],
        [1.2050],
        [1.2067],
        [1.2176],
        [1.1717],
        [1.2025],
        [1.1963],
        [1.1858],
        [1.1737],
        [1.2007],
        [1.1719],
        [1.1772],
        [1.2466],
        [1.1906],
        [1.2087],
        [1.1512],
        [1.1764],
        [1.1285],
        [1.1923],
        [1.2176],
        [1.2335],
        [1.1648],
        [1.1527],
        [1.1756],
        [1.1957],
        [1.1549],
        [1.1780],
        [1.1324],
        [1.1871],
        [1.1871],
        [1.1971],
        [1.1533],
        [1.1984],
        [1.1752],
        [1.1767],
        [1.1794],
        [1.1798],
        [1.2538],
        [1.2139],
        [1.1692],
        [1.1858],
        [1.1923],
        [1.2138],
        [1.1754],
        [1.1714],
        [1.1686],
        [1.1853],
        [1.2221],
        [1.1861],
        [1.2211],
        [1.1878],
        [1.1753],
        [1.1983],
        [1.1992],
        [1.2359],
        [1.1589],
        [1.2164],
        [1.1532],
        [1.2051],
        [1.1698],
        [1.1962],
        [1.1824],
        [1.1641],
        [1.1820],
        [1.1536],
        [1.2364],
        [1.1915],
        [1.1785],
        [1.1881],
        [1.1551],
        [1.1759],
        [1.1860],
        [1.1770],
        [1.2184],
        [1.2371],
        [1.1376],
        [1.2388],
        [1.1350],
        [1.1910],
        [1.2029],
        [1.1834],
        [1.1786],
        [1.2168],
        [1.1585],
        [1.1992],
        [1.2389],
        [1.1510],
        [1.2063],
        [1.1875],
        [1.1953],
        [1.1657],
        [1.1813],
        [1.2109],
        [1.2206],
        [1.2259],
        [1.1555],
        [1.1575],
        [1.2177]])
cost_pts shape:  torch.Size([200, 37])
cost_pts:  tensor([[0.6055, 0.4416, 0.8190,  ..., 0.2208, 0.6207, 0.9999],
        [0.9398, 0.5423, 1.1533,  ..., 0.9662, 0.2433, 0.6881],
        [0.1309, 0.5272, 0.0838,  ..., 0.5444, 0.8262, 0.5666],
        ...,
        [0.6103, 0.4878, 0.4988,  ..., 0.3384, 0.6669, 1.0461],
        [0.3620, 0.1512, 0.5755,  ..., 0.4017, 0.3346, 0.7094],
        [0.5582, 0.6807, 0.6697,  ..., 1.2335, 0.6453, 0.2005]])
cost torch.Size([200, 37])
indices0 shape:  37
outlabel_shape:  torch.Size([200, 1])
src_logit shape:  torch.Size([200, 1])
tgt_cls shape:  torch.Size([200])
tgt_pts_shape:  torch.Size([37, 2])
{'loss_ce': tensor(5.2500, device='cuda:0', grad_fn=<MulBackward0>), 'loss_center': tensor(0.0007, device='cuda:0', grad_fn=<DivBackward0>)}
after merge:  torch.Size([1, 128, 80, 125])
query shape:  torch.Size([1, 200, 128])
ref pts dec:  torch.Size([1, 200, 2])
out_prob shape:  torch.Size([200, 1])
pos_c_cls:  torch.Size([200, 1])
tgt_ids:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])
cost_class shape:  torch.Size([200, 1])
cost_class:  tensor([[1.1835],
        [1.2042],
        [1.1886],
        [1.1680],
        [1.1901],
        [1.2633],
        [1.2238],
        [1.2092],
        [1.1798],
        [1.2015],
        [1.2170],
        [1.2359],
        [1.2467],
        [1.2020],
        [1.1945],
        [1.1869],
        [1.1858],
        [1.1668],
        [1.1977],
        [1.1898],
        [1.2445],
        [1.2133],
        [1.1982],
        [1.1365],
        [1.1675],
        [1.1780],
        [1.1685],
        [1.2376],
        [1.1497],
        [1.1623],
        [1.2279],
        [1.1593],
        [1.2349],
        [1.1997],
        [1.2034],
        [1.2392],
        [1.1769],
        [1.2085],
        [1.1723],
        [1.1884],
        [1.1309],
        [1.2058],
        [1.2140],
        [1.2241],
        [1.2114],
        [1.2326],
        [1.1562],
        [1.1754],
        [1.2287],
        [1.2294],
        [1.1496],
        [1.1747],
        [1.2260],
        [1.1797],
        [1.1947],
        [1.2123],
        [1.2098],
        [1.1952],
        [1.2517],
        [1.2214],
        [1.1849],
        [1.2047],
        [1.2439],
        [1.2047],
        [1.1765],
        [1.2007],
        [1.1848],
        [1.2168],
        [1.1481],
        [1.2137],
        [1.2230],
        [1.2193],
        [1.1408],
        [1.2200],
        [1.2155],
        [1.1635],
        [1.1999],
        [1.2110],
        [1.2362],
        [1.1953],
        [1.2117],
        [1.2307],
        [1.1788],
        [1.1485],
        [1.1499],
        [1.2238],
        [1.1824],
        [1.1510],
        [1.1853],
        [1.2043],
        [1.1863],
        [1.2075],
        [1.2089],
        [1.1837],
        [1.1921],
        [1.2081],
        [1.2047],
        [1.1415],
        [1.1893],
        [1.1192],
        [1.1604],
        [1.2081],
        [1.2049],
        [1.1997],
        [1.2448],
        [1.2105],
        [1.2573],
        [1.1696],
        [1.1389],
        [1.2016],
        [1.1830],
        [1.2008],
        [1.1747],
        [1.2189],
        [1.2019],
        [1.2005],
        [1.1964],
        [1.2122],
        [1.2249],
        [1.1711],
        [1.1341],
        [1.1781],
        [1.1861],
        [1.2032],
        [1.1824],
        [1.2381],
        [1.2694],
        [1.2382],
        [1.2077],
        [1.2096],
        [1.1377],
        [1.1871],
        [1.2161],
        [1.2251],
        [1.1937],
        [1.2293],
        [1.2091],
        [1.1912],
        [1.2368],
        [1.1881],
        [1.2002],
        [1.1623],
        [1.2243],
        [1.2388],
        [1.1703],
        [1.2030],
        [1.1824],
        [1.2305],
        [1.2387],
        [1.1819],
        [1.1554],
        [1.1438],
        [1.1942],
        [1.1869],
        [1.1845],
        [1.1779],
        [1.2508],
        [1.2404],
        [1.2103],
        [1.1784],
        [1.2267],
        [1.1661],
        [1.2212],
        [1.1981],
        [1.1596],
        [1.1406],
        [1.1573],
        [1.1915],
        [1.2173],
        [1.2716],
        [1.2063],
        [1.2124],
        [1.1616],
        [1.1769],
        [1.1906],
        [1.2367],
        [1.2116],
        [1.1987],
        [1.2146],
        [1.2026],
        [1.1857],
        [1.2004],
        [1.2091],
        [1.1975],
        [1.1869],
        [1.1982],
        [1.1864],
        [1.1961],
        [1.2036],
        [1.2564],
        [1.1918],
        [1.1869],
        [1.2365],
        [1.2318],
        [1.1699],
        [1.2174],
        [1.1912],
        [1.1694],
        [1.1669],
        [1.1538]])
cost_pts shape:  torch.Size([200, 41])
cost_pts:  tensor([[0.7640, 0.4769, 0.6841,  ..., 0.5589, 0.9323, 1.0111],
        [1.0983, 0.5651, 0.1710,  ..., 0.2971, 0.7236, 1.3454],
        [0.0644, 0.5044, 1.1316,  ..., 0.7724, 0.4991, 0.2760],
        ...,
        [0.5438, 0.5231, 0.9556,  ..., 0.6051, 0.9785, 0.5347],
        [0.5205, 0.1864, 0.6399,  ..., 0.2808, 0.6418, 0.7676],
        [0.6247, 0.6455, 0.9507,  ..., 0.5915, 0.1900, 0.6339]])
cost torch.Size([200, 41])
indices0 shape:  41
outlabel_shape:  torch.Size([200, 1])
src_logit shape:  torch.Size([200, 1])
tgt_cls shape:  torch.Size([200])
tgt_pts_shape:  torch.Size([41, 2])
{'loss_ce': tensor(4.6704, device='cuda:0', grad_fn=<MulBackward0>), 'loss_center': tensor(0.0007, device='cuda:0', grad_fn=<DivBackward0>)}
after merge:  torch.Size([1, 128, 80, 125])
query shape:  torch.Size([1, 200, 128])
ref pts dec:  torch.Size([1, 200, 2])
out_prob shape:  torch.Size([200, 1])
pos_c_cls:  torch.Size([200, 1])
tgt_ids:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])
cost_class shape:  torch.Size([200, 1])
cost_class:  gt_cnt:  38
gt_cnt:  35
gt_cnt:  40
gt_cnt:  39
gt_cnt:  39
gt_cnt:  41
gt_cnt:  41
gt_cnt:  39
gt_cnt:  37
gt_cnt:  37
gt_cnt:  41
gt_cnt:  38
tensor([[1.2076],
        [1.1363],
        [1.1855],
        [1.1557],
        [1.1669],
        [1.1762],
        [1.1916],
        [1.2011],
        [1.2106],
        [1.2097],
        [1.1393],
        [1.1590],
        [1.1980],
        [1.1454],
        [1.1296],
        [1.1892],
        [1.1663],
        [1.2133],
        [1.1484],
        [1.1228],
        [1.1840],
        [1.1203],
        [1.2052],
        [1.1525],
        [1.1914],
        [1.1872],
        [1.1863],
        [1.1302],
        [1.1554],
        [1.1701],
        [1.2575],
        [1.1640],
        [1.1627],
        [1.1947],
        [1.1528],
        [1.1936],
        [1.1795],
        [1.1262],
        [1.1780],
        [1.1538],
        [1.2154],
        [1.2115],
        [1.1708],
        [1.1548],
        [1.1966],
        [1.1460],
        [1.1558],
        [1.1347],
        [1.2011],
        [1.2283],
        [1.1720],
        [1.1783],
        [1.1884],
        [1.1469],
        [1.2014],
        [1.1700],
        [1.2036],
        [1.1892],
        [1.1896],
        [1.2314],
        [1.1647],
        [1.1784],
        [1.2209],
        [1.1948],
        [1.2157],
        [1.1540],
        [1.2144],
        [1.1767],
        [1.1843],
        [1.2259],
        [1.1847],
        [1.2256],
        [1.1899],
        [1.1905],
        [1.2541],
        [1.2213],
        [1.1984],
        [1.1537],
        [1.1284],
        [1.2211],
        [1.1720],
        [1.1560],
        [1.1716],
        [1.2132],
        [1.1919],
        [1.1995],
        [1.1644],
        [1.1687],
        [1.1865],
        [1.1947],
        [1.1871],
        [1.1289],
        [1.1893],
        [1.2058],
        [1.2168],
        [1.1815],
        [1.2217],
        [1.2099],
        [1.1713],
        [1.2114],
        [1.1860],
        [1.2058],
        [1.1398],
        [1.1513],
        [1.1609],
        [1.1896],
        [1.2055],
        [1.1402],
        [1.1674],
        [1.1565],
        [1.1834],
        [1.1957],
        [1.2126],
        [1.1488],
        [1.1769],
        [1.1729],
        [1.1263],
        [1.2063],
        [1.1463],
        [1.1460],
        [1.1867],
        [1.1832],
        [1.1976],
        [1.1351],
        [1.1453],
        [1.1922],
        [1.1718],
        [1.1676],
        [1.1887],
        [1.1757],
        [1.1900],
        [1.2251],
        [1.1802],
        [1.2331],
        [1.2159],
        [1.1831],
        [1.2510],
        [1.2338],
        [1.1659],
        [1.2098],
        [1.1943],
        [1.1998],
        [1.1679],
        [1.1508],
        [1.1707],
        [1.1949],
        [1.1659],
        [1.1865],
        [1.2114],
        [1.2006],
        [1.1884],
        [1.1296],
        [1.1643],
        [1.1326],
        [1.1934],
        [1.1957],
        [1.2018],
        [1.1991],
        [1.1560],
        [1.2147],
        [1.1620],
        [1.1660],
        [1.1725],
        [1.1981],
        [1.1921],
        [1.2025],
        [1.1851],
        [1.1688],
        [1.1679],
        [1.1567],
        [1.2121],
        [1.1678],
        [1.2031],
        [1.1744],
        [1.1990],
        [1.1572],
        [1.2351],
        [1.1631],
        [1.1635],
        [1.1584],
        [1.1791],
        [1.1446],
        [1.2121],
        [1.1611],
        [1.1783],
        [1.1809],
        [1.2301],
        [1.1760],
        [1.1550],
        [1.1739],
        [1.1868],
        [1.1720],
        [1.1839],
        [1.1628],
        [1.1731],
        [1.2146],
        [1.1265],
        [1.1922],
        [1.2085],
        [1.1925]])
cost_pts shape:  torch.Size([200, 38])
cost_pts:  tensor([[0.6501, 0.5752, 0.5734,  ..., 0.7736, 0.9488, 0.3276],
        [0.8539, 0.2798, 0.9078,  ..., 0.4243, 1.2832, 0.9750],
        [0.2168, 0.8467, 0.1617,  ..., 0.6452, 0.2137, 0.5532],
        ...,
        [0.6963, 0.6707, 0.5064,  ..., 0.8198, 0.6350, 0.2315],
        [0.3596, 0.3551, 0.3299,  ..., 0.4832, 0.7054, 0.4105],
        [0.4723, 0.6658, 0.6622,  ..., 0.4643, 0.5336, 1.2424]])
cost torch.Size([200, 38])
indices0 shape:  38
outlabel_shape:  torch.Size([200, 1])
src_logit shape:  torch.Size([200, 1])
tgt_cls shape:  torch.Size([200])
tgt_pts_shape:  torch.Size([38, 2])
{'loss_ce': tensor(5.0520, device='cuda:0', grad_fn=<MulBackward0>), 'loss_center': tensor(0.0006, device='cuda:0', grad_fn=<DivBackward0>)}
