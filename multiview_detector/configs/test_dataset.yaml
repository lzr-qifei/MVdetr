SUPER_CONFIG_PATH:

MODE:       # "train" or "eval" or "submit", for the main.py script.

# System config, like CPU/GPU
NUM_CPU_PER_GPU:          # number of CPU per GPU
NUM_WORKERS: 4
DEVICE: cuda
AVAILABLE_GPUS: "0"

# Git version:
GIT_VERSION:              # you should input the git version here, if you are using wandb to log your experiments.

# Datasets:
DATASET: [MultiviewX]    # for joint training, there may be multiple datasets, like: [CrowdHuman, MOT17]
TRAIN_RATIO: 0.9
LENGTH_PER_SEQUENCE: 5
# DATASET_SPLITS: [train]   # and corresponding splits, like: [train, val]
DATA_ROOT: /root/autodl-tmp/   # datasets root
# Sampling settings:
SAMPLE_STEPS: [0]
SAMPLE_LENGTHS: [5]
SAMPLE_MODES: [random_interval]
SAMPLE_INTERVALS: [1]

#Reduce settings:
WORLD_REDUCE: 4
BOTTLENECK_DIM: 128
WORLD_KERNEL_SIZE: 10
IMG_REDUCE: 12
IMG_KERNEL_SIZE: 10
SEMI_SUPERVISED: 0
DROPCAM: 0.0
AUGMENTATION: True


# Training Setting:
TRAIN_STAGE: joint
SEED: 42
USE_DISTRIBUTED: False
DETR_NUM_TRAIN_FRAMES: 4
# Below two parameters are for memory optimized DETR training:
DETR_CHECKPOINT_FRAMES: 5
SEQ_DECODER_CHECKPOINT: False
# Training Augmentation:
TRAJ_DROP_RATIO: 0.5
TRAJ_SWITCH_RATIO: 0.3
# Training Scheduler:
EPOCHS: 14
LR: 1.0e-4
LR_BACKBONE_NAMES: [backbone.0]
LR_BACKBONE_SCALE: 0.1
LR_LINEAR_PROJ_NAMES: [reference_points, sampling_offsets]
LR_LINEAR_PROJ_SCALE: 0.05
LR_WARMUP_EPOCHS: 1
WEIGHT_DECAY: 0.0005
CLIP_MAX_NORM: 0.1
SCHEDULER_TYPE: MultiStep
SCHEDULER_MILESTONES: [8, 12]
SCHEDULER_GAMMA: 0.1
BATCH_SIZE: 1
ACCUMULATE_STEPS: 2
RESUME_MODEL:
RESUME_OPTIMIZER: True
RESUME_SCHEDULER: True
RESUME_STATES: True